{"cells":[{"cell_type":"markdown","source":["# Colab Notebook for SAM2 segmentation\n","\n","Author: Dr.Yusuke Kimata (ORCID: https://orcid.org/0000-0002-1366-0636)<br>\n","ref: Notebook by Meta Research (https://github.com/facebookresearch/sam2/blob/main/notebooks/)<br>\n","\n","### **Revision for flexible reference frame selection (2025/09/22)**\n","For initial segmentation, you can select not only the first frame but also any other frame as the reference.\n","\n","### **Revision for new model SAM 2.1 (2025/02/05)**\n","Release Note (https://github.com/facebookresearch/sam2/blob/main/RELEASE_NOTES.md)\n"],"metadata":{"id":"-JS67SsceFGB"}},{"cell_type":"markdown","source":["This notebook is for segmentation of time-lapse image with SAM2\n","\n","Set `/content/drive/MyDrive/Colab Notebooks/SAM2-segmentation/` as the working directory.<br>\n","Place the JPEG files, each corresponding to a separate frame, into the input directory.  <br>\n","Only JPEG files with numeric filenames can be processed as input (e.g., `000.jpg`, `001.jpg`, ...).  <br>\n","Make sure to change the runtime type to **\"GPU\"**"],"metadata":{"id":"JXpEH0LgmU5F"}},{"cell_type":"markdown","metadata":{"id":"FiV18tHu90V1"},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XWepIoARdm5r"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/MyDrive/\n","\n","import os\n","HOME = os.getcwd()\n","print(\"HOME:\", HOME)\n","\n","using_colab = True\n","if using_colab:\n","    import torch\n","    import torchvision\n","    print(\"PyTorch version:\", torch.__version__)\n","    print(\"Torchvision version:\", torchvision.__version__)\n","    print(\"CUDA is available:\", torch.cuda.is_available())\n","    !git clone https://github.com/facebookresearch/sam2.git # Comment out this line on subsequent runs"]},{"cell_type":"code","source":["%cd {HOME}/sam2"],"metadata":{"id":"h5anHEUz1Lb9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m3kLUcUXf-00"},"outputs":[],"source":["!pip install -q supervision\n","!pip install -q hydra-core\n","!pip install -q iopath\n","!pip install -q jupyter_bbox_widget"]},{"cell_type":"markdown","metadata":{"id":"V-50ZjJA-MPl"},"source":["### Download SAM2 checkpoints\n","\n","**NOTE:** SAM2.1 is available in 4 different model sizes ranging from the lightweight \"sam2.1_hiera_tiny\" (38.9M parameters) to the more powerful \"sam2.1_hiera_large\" (224.4M parameters)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tgf2WKiO-ECI"},"outputs":[],"source":["# Not required on subsequent runs\n","!mkdir -p {HOME}/checkpoints\n","!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_tiny.pt -P {HOME}/checkpoints\n","!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_small.pt -P {HOME}/checkpoints\n","!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_base_plus.pt -P {HOME}/checkpoints\n","!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt -P {HOME}/checkpoints"]},{"cell_type":"markdown","metadata":{"id":"EbD4Hv0q-Rmt"},"source":["### Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-HL-09Fj-OdY"},"outputs":[],"source":["import cv2\n","import torch\n","import base64\n","\n","import json\n","\n","import numpy as np\n","import supervision as sv\n","\n","import matplotlib.pyplot as plt\n","from glob import glob\n","from natsort import natsorted\n","from google.colab.patches import cv2_imshow\n","\n","from pathlib import Path\n","from supervision.assets import download_assets, VideoAssets\n","from sam2.build_sam import build_sam2_video_predictor\n","\n","from sam2.build_sam import build_sam2\n","from sam2.sam2_image_predictor import SAM2ImagePredictor\n","from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n","\n","from tqdm import tqdm\n","\n","IS_COLAB = True\n","\n","if IS_COLAB:\n","    from google.colab import output\n","    output.enable_custom_widget_manager()\n","\n","from jupyter_bbox_widget import BBoxWidget"]},{"cell_type":"markdown","metadata":{"id":"GYQV0xYu-W16"},"source":["**NOTE:** This code enables mixed-precision computing for faster deep learning. It uses bfloat16 for most calculations and, on newer NVIDIA GPUs, leverages TensorFloat-32 (TF32) for certain operations to further boost performance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G_ca4kbz-UlP"},"outputs":[],"source":["torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n","\n","if torch.cuda.get_device_properties(0).major >= 8:\n","    torch.backends.cuda.matmul.allow_tf32 = True\n","    torch.backends.cudnn.allow_tf32 = True"]},{"cell_type":"markdown","source":["### Prepare functions"],"metadata":{"id":"PfSYtXSNrEh0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"D1Pm_sqd-a4A"},"outputs":[],"source":["def encode_image(filepath):\n","    with open(filepath, 'rb') as f:\n","        image_bytes = f.read()\n","    encoded = str(base64.b64encode(image_bytes), 'utf-8')\n","    return \"data:image/jpg;base64,\"+encoded\n","\n","# Function to visualize a mask\n","def show_mask(mask, ax, obj_id=None):\n","    cmap = plt.get_cmap(\"tab10\")\n","    cmap_idx = 0 if obj_id is None else obj_id\n","    color = np.array([*cmap(cmap_idx)[:3], 0.3])\n","\n","    h, w = mask.shape[-2:]\n","    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n","    ax.imshow(mask_image)\n","\n","# Function to visualize prompt points\n","def show_points(coords, labels, ax, marker_size=30):\n","    pos_points = coords[labels==1]\n","    neg_points = coords[labels==0]\n","    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='o', s=marker_size, edgecolor='white', linewidth=1.25)\n","    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='x', s=marker_size, linewidth=1.25)\n","\n","# Function to visualize prompt boxes\n","def show_box(box, ax):\n","    x0, y0 = box[0], box[1]\n","    w, h = box[2] - box[0], box[3] - box[1]\n","    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))\n","\n","# Function to overlay a mask on the original image\n","def show_mask_cv2(image, mask, obj_id=None):\n","    cmap = plt.get_cmap(\"tab10\")\n","    cmap_idx = 0 if obj_id is None else obj_id\n","    color = np.array([*cmap(cmap_idx)[:3], 0.3]) * 255\n","    h, w = mask.shape[-2:]\n","\n","    mask_image = (mask.reshape(h, w, 1) * color[:3]).astype(np.uint8)\n","\n","    overlay = cv2.addWeighted(image, 1, mask_image, 0.2, 0)\n","    return overlay\n","\n","# Function to save a mask as an 8-bit grayscale image\n","def save_mask_grayscale(mask, output_path):\n","    h, w = mask.shape[-2:]\n","\n","    mask_image = (mask.reshape(h, w) * 255).astype(np.uint8)\n","\n","    cv2.imwrite(output_path, mask_image)\n","\n","# Fuction for propagation of inference state\n","def collect_results(gen, video_segments):\n","    for f_idx, obj_ids, logits in gen:\n","        video_segments.setdefault(f_idx, {})\n","        for i, oid in enumerate(obj_ids):\n","            video_segments[f_idx][oid] = (logits[i] > 0).cpu().numpy()"]},{"cell_type":"markdown","metadata":{"id":"b2a5NdUA_drI"},"source":["## Load model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oXn6kAwP-YyE"},"outputs":[],"source":["DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","CHECKPOINT = f\"{HOME}/checkpoints/sam2.1_hiera_large.pt\"\n","CONFIG = \"configs/sam2.1/sam2.1_hiera_l.yaml\""]},{"cell_type":"markdown","metadata":{"id":"vuoGHyB-2MM4"},"source":["## Segmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZD2VRKXx940O"},"outputs":[],"source":["# Prepare model\n","predictor = build_sam2_video_predictor(CONFIG, CHECKPOINT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q3uq4pC3mHA4"},"outputs":[],"source":["# Input, output directory name\n","input_name = \"input_MT_demo\"\n","output_name = \"output_MT_demo\"\n","\n","input_img_dir = \"/content/drive/MyDrive/Colab Notebooks/SAM2-segmentation/\" + input_name + \"/\"\n","output_mask_dir = \"/content/drive/MyDrive/Colab Notebooks/SAM2-segmentation/\" + output_name + \"_mask/\"\n","output_merged_dir = \"/content/drive/MyDrive/Colab Notebooks/SAM2-segmentation/\" + output_name + \"_merged/\"\n","\n","# Create the directory if it does not exist\n","os.makedirs(input_img_dir, exist_ok=True)\n","os.makedirs(output_mask_dir, exist_ok=True)\n","os.makedirs(output_merged_dir, exist_ok=True)\n","\n","# Check\n","print(f\"Input directory: {os.path.exists(input_img_dir)}\")\n","print(f\"Mask directory: {os.path.exists(output_mask_dir)}\")\n","print(f\"Merged directory: {os.path.exists(output_merged_dir)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a4zic4utnIRg"},"outputs":[],"source":["# Get the first frame of the video\n","frame_names = natsorted(glob(f\"{input_img_dir}*.jpg\"))\n","img = cv2.imread(frame_names[0])\n","img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","# Visualization\n","plt.figure(figsize=(4, 4))\n","plt.imshow(img)\n","plt.axis('on')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"6gPNBqn5Oi1G"},"source":["### Initialize the inference state\n","\n","**NOTE:** SAM 2 requires stateful inference for interactive video segmentation, so we need to initialize an inference state on this video. During initialization, it loads all the JPEG frames in `video_path` and stores their pixels in `inference_state` (as shown in the progress bar below)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4bo5lxS4q53a"},"outputs":[],"source":["# Always run this code before processing a new video\n","inference_state = predictor.init_state(video_path=input_img_dir)"]},{"cell_type":"markdown","metadata":{"id":"fqD4FFL4swI7"},"source":["**NOTE:** If you have run any previous tracking using this inference_state, please reset it first via reset_state. (The cell below is just for illustration; it's not needed to call reset_state here as this inference_state is just freshly initialized above.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fwa_-C7SswJI"},"outputs":[],"source":["# Always run this before processing a new video\n","predictor.reset_state(inference_state)"]},{"cell_type":"markdown","metadata":{"id":"bb9PmRFGs1v2"},"source":["### Prompting with points"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-hBk4UPcs1v3"},"outputs":[],"source":["OBJECTS = ['object', 'bg'] # Define target object as 'object' and background as 'bg'"]},{"cell_type":"markdown","metadata":{"id":"Iz-QaOA-s1v3"},"source":["**NOTE:** Let's choose the index of the reference frame that we will use to annotate the objects we are looking for."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LbsOYYJls1v3"},"outputs":[],"source":["FRAME_IDX = 10 # Set the frame idx of reference for initial segmentation\n","OBJECT_ID = 1\n","FRAME_PATH = f\"{input_img_dir}{FRAME_IDX:03d}.jpg\"\n","\n","widget = BBoxWidget(classes=OBJECTS)\n","widget.image = encode_image(FRAME_PATH)\n","widget\n","# Click on several points in the displayed widget to indicate the object to segment and the background.\n","# Don't worry if the displayed points appear slightly misaligned."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3fq8tHY7xDaE"},"outputs":[],"source":["# Retrieve bounding box coordinates from widget.bboxes\n","coordinates = widget.bboxes\n","\n","# Generate a filename using the name of the input directory\n","dir_name = os.path.basename(os.path.normpath(input_img_dir))  # Extract only the directory name\n","os.makedirs(\"/content/drive/MyDrive/Colab Notebooks/SAM2-segmentation/prompts/\", exist_ok=True)\n","output_file = \"/content/drive/MyDrive/Colab Notebooks/SAM2-segmentation/prompts/\" + f'coordinates_{dir_name}_frame{FRAME_IDX}.txt'\n","\n","# Save the coordinates to a text file\n","with open(output_file, 'w') as file:\n","    # Write the FRAME_PATH as the first line\n","    file.write(f'FRAME_PATH = \"{FRAME_PATH}\"\\n')\n","\n","    # Save the coordinate data in JSON format\n","    json.dump(coordinates, file, indent=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VsbJwTCC38ry"},"outputs":[],"source":["widget.bboxes # Copy the output to 'default_box' in the next cell, or skip this cell."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JLc4E7h7HfOG"},"outputs":[],"source":["default_box = []\n","\n","boxes = widget.bboxes if widget.bboxes else default_box\n","input_point = np.array([\n","    [\n","        box['x'],\n","        box['y']\n","    ] for box in boxes\n","])\n","# Assign 0 to the 'bg' label and 1 to all other labels\n","input_label = np.array([\n","    0 if box['label'] == 'bg' else 1 for box in boxes\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nJu5HkNXxDaG"},"outputs":[],"source":["# Perform segmentation on the specified frame using the input prompts\n","_, out_obj_ids, out_mask_logits = predictor.add_new_points(\n","    inference_state=inference_state,\n","    frame_idx=FRAME_IDX,\n","    obj_id=OBJECT_ID,\n","    points=input_point,\n","    labels=input_label,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sVbwi_Rp-fyL"},"outputs":[],"source":["# Visualize the result\n","plt.figure(figsize=(6, 6))\n","plt.title(f\"frame {FRAME_IDX}\")\n","image = cv2.imread(frame_names[FRAME_IDX])\n","image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","plt.imshow(image)\n","show_points(input_point, input_label, plt.gca())\n","show_mask((out_mask_logits[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])"]},{"cell_type":"code","source":["# Run segmentation on all frames of the video\n","video_segments = {}\n","\n","# Forward (FRAME_IDX→last)\n","for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(\n","        inference_state,\n","        start_frame_idx=FRAME_IDX,\n","        reverse=False\n","    ):\n","    if out_frame_idx not in video_segments:\n","        video_segments[out_frame_idx] = {}\n","    for i, out_obj_id in enumerate(out_obj_ids):\n","        video_segments[out_frame_idx][out_obj_id] = (out_mask_logits[i] > 0).cpu().numpy()\n","\n","# Backward (FRAME_IDX→start）)\n","for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(\n","        inference_state,\n","        start_frame_idx=FRAME_IDX,\n","        reverse=True\n","    ):\n","    if out_frame_idx not in video_segments:\n","        video_segments[out_frame_idx] = {}\n","    for i, out_obj_id in enumerate(out_obj_ids):\n","        video_segments[out_frame_idx][out_obj_id] = (out_mask_logits[i] > 0).cpu().numpy()"],"metadata":{"id":"8GcF2_UE7zj7"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xLQxnL2dwVTE"},"outputs":[],"source":["# Display every few frames to check the results\n","vis_frame_stride = 5 # step interval\n","\n","plt.close(\"all\")\n","for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n","    image = cv2.imread(frame_names[out_frame_idx])\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n","        image = show_mask_cv2(image, out_mask, obj_id=out_obj_id)\n","        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","        print(f\"frame {out_frame_idx}\")\n","        cv2_imshow(image)"]},{"cell_type":"markdown","metadata":{"id":"adQoEqhnC9S9"},"source":["## Addition of Reference Frames (Optional)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rHHgM53nucyg"},"outputs":[],"source":["# Retrieve a different frame from the video\n","FRAME_IDX2 = 25\n","OBJECT_ID = 1\n","img2 = cv2.imread(frame_names[FRAME_IDX2])\n","img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n","\n","# Visualization\n","plt.figure(figsize=(4, 4))\n","plt.imshow(img2)\n","plt.axis('on')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3hhpBQ9cuTxN"},"outputs":[],"source":["FRAME_PATH2 = f\"{input_img_dir}{FRAME_IDX2:03d}.jpg\"\n","\n","widget2 = BBoxWidget(classes=OBJECTS)\n","widget2.image = encode_image(FRAME_PATH2)\n","widget2\n","# Click on several points in the displayed widget to indicate the object to segment and the background.\n","# Don't worry if the displayed points appear slightly misaligned."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vkjFBuklFEDA"},"outputs":[],"source":["# Retrieve bounding box coordinates from widget2.bboxes\n","coordinates = widget2.bboxes\n","\n","# Generate a filename using the name of the input directory\n","dir_name = os.path.basename(os.path.normpath(input_img_dir))  # Extract directory name only\n","output_file = \"/content/drive/MyDrive/Colab Notebooks/SAM2-segmentation/prompts/\" + f'coordinates_{dir_name}_frame{FRAME_IDX2}_.txt'\n","\n","# Save the coordinates to a text file\n","with open(output_file, 'w') as file:\n","    # Write the FRAME_PATH as the first line\n","    file.write(f'FRAME_PATH = \"{FRAME_PATH2}\"\\n')\n","\n","    # Save the coordinate data in JSON format\n","    json.dump(coordinates, file, indent=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aL-VkJwHt_NE"},"outputs":[],"source":["default_box = []\n","\n","boxes = widget2.bboxes if widget2.bboxes else default_box\n","input_point = np.array([\n","    [\n","        box['x'],\n","        box['y']\n","    ] for box in boxes\n","])\n","# Assign 0 to the 'bg' label and 1 to all other labels\n","input_label = np.array([\n","    0 if box['label'] == 'bg' else 1 for box in boxes\n","])\n","\n","_, out_obj_ids, out_mask_logits = predictor.add_new_points(\n","    inference_state=inference_state,\n","    frame_idx=FRAME_IDX2,\n","    obj_id=OBJECT_ID,\n","    points=input_point,\n","    labels=input_label,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kYDoo2Bp2h3x"},"outputs":[],"source":["plt.figure(figsize=(6, 6))\n","plt.title(f\"frame {FRAME_IDX2}\")\n","image = cv2.imread(frame_names[FRAME_IDX2])\n","image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","plt.imshow(image)\n","show_points(input_point, input_label, plt.gca())\n","show_mask((out_mask_logits[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KUscKxK14tNE"},"outputs":[],"source":["# Run segmentation on all frames of the video\n","video_segments = {}\n","\n","collect_results(\n","    predictor.propagate_in_video(inference_state,\n","                                 start_frame_idx=FRAME_IDX2,\n","                                 reverse=False),\n","    video_segments\n",")\n","\n","collect_results(\n","    predictor.propagate_in_video(inference_state,\n","                                 start_frame_idx=FRAME_IDX2,\n","                                 reverse=True),\n","    video_segments\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x13U8etO40Qd"},"outputs":[],"source":["# Display every few frames to check the results\n","vis_frame_stride = 5 # step interval\n","\n","plt.close(\"all\")\n","for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n","    image = cv2.imread(frame_names[out_frame_idx])\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n","        image = show_mask_cv2(image, out_mask, obj_id=out_obj_id)\n","        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","        print(f\"frame {out_frame_idx}\")\n","        cv2_imshow(image)"]},{"cell_type":"markdown","metadata":{"id":"vuY44QQ6CyUl"},"source":["## Save results as mask images\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yePgCglUhcPo"},"source":["### (Option1) Saving the Masks in Grayscale"]},{"cell_type":"code","source":["for out_frame_idx in tqdm(range(len(frame_names)), desc=\"Saving masks\"):\n","    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n","        basename = os.path.basename(frame_names[out_frame_idx])\n","        output_frame = os.path.join(output_mask_dir, basename)\n","\n","        # Save the mask as an 8-bit grayscale image\n","        save_mask_grayscale(out_mask, output_frame)"],"metadata":{"id":"khbRu60XtGBo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qA63IJdyhf66"},"source":["### (Option2) Saving Overlayed Masks on Original Images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4_LJDHY91RzL"},"outputs":[],"source":["for out_frame_idx in tqdm(range(len(frame_names)), desc=\"Saving overlayed images\"):\n","    # Load the original image\n","    image = cv2.imread(frame_names[out_frame_idx])\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","    # Overlay masks onto the image\n","    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n","        image = show_mask_cv2(image, out_mask, obj_id=out_obj_id)\n","\n","    # Prepare output path\n","    basename = os.path.basename(frame_names[out_frame_idx])\n","    output_frame = os.path.join(output_merged_dir, basename)\n","\n","    # Save the resulting image\n","    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","    cv2.imwrite(output_frame, image)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1ln0MLPteIubDvpp0pfkkndxBlKWU4od6","timestamp":1738733596879}],"authorship_tag":"ABX9TyPRAUbXD+h21Jtc5yxJhCks"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}